{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDy9Gm/P3zf4bcvGBvfl1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedMostafa3m/NLP/blob/main/dataset_generator/MyOwn_data_set_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar00Qn_xyP-7",
        "outputId": "e9f24eb6-517d-4caa-d22e-76b4f91acd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "mBLke-s7ypI-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      device_map=\"auto\",\n",
        "      quantization_config=quant_config\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "YYeW93qvyo_P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model(\"meta-llama/Llama-3.2-1B-Instruct\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SMH_UP23yovz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(topic, number_of_data, inq1, resp1, inq2, resp2):\n",
        "    # Convert user inputs into multi-shot examples\n",
        "    multi_shot_examples = [\n",
        "        {\"inquiry\": inq1, \"response\": resp1},\n",
        "        {\"inquiry\": inq2, \"response\": resp2}\n",
        "    ]\n",
        "\n",
        "    # System prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a helpful assistant whose main purpose is to generate datasets.\n",
        "    Topic: {topic}\n",
        "    Return the dataset in JSON format. Use examples with simple, and easy-to-understand inquiry for foreigners.\n",
        "    Include the following examples: {multi_shot_examples}\n",
        "    Return {number_of_data} examples each time.\n",
        "    Do not repeat the provided examples.\n",
        "    \"\"\"\n",
        "        # Example Messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Please generate my dataset for {topic}\"}\n",
        "    ]\n",
        "\n",
        "    # Tokenize Input\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Generate Output\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "\n",
        "    # Decode and Return\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "H9UaqXaP1AJf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_interface(topic, number_of_data, inq1, resp1, inq2, resp2):\n",
        "    return generate_dataset(topic, number_of_data, inq1, resp1, inq2, resp2)"
      ],
      "metadata": {
        "id": "iTAUK7X-7VgT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_topic = \"Talking to foreigners explain to them Ancient Egyptian monuments.\"\n",
        "default_number_of_data = 5\n",
        "default_multi_shot_examples = [\n",
        "  {\n",
        "    \"inquiry\": \"Give safety and cultural tips for tourists visiting ancient Egyptian sites.\",\n",
        "    \"response\": \"Wear comfortable clothing and shoes suitable for walking on uneven ground. Always carry water, sunscreen, and a hat to protect yourself from the sun. Respect local customs by dressing modestly, avoid touching or climbing on monuments, and follow guidance from local authorities or tour guides.\"\n",
        "  },\n",
        "  {\n",
        "    \"inquiry\": \"Explain why visiting the Valley of the Kings is a must for history lovers.\",\n",
        "    \"response\": \"The Valley of the Kings offers visitors a chance to step directly into the tombs of ancient pharaohs, including Tutankhamun. The vivid wall paintings and hieroglyphs reveal stories of the afterlife and the beliefs of ancient Egyptians. It’s an extraordinary place for anyone fascinated by archaeology and ancient history.\"\n",
        "  },\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "LMRXAdkQ7Xs0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr_interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Topic\", value=default_topic, lines=2),\n",
        "        gr.Number(label=\"Number of Examples\", value=default_number_of_data, precision=0),\n",
        "        gr.Textbox(label=\"inquiry 1\", value=default_multi_shot_examples[0][\"inquiry\"]),\n",
        "        gr.Textbox(label=\"Response 1\", value=default_multi_shot_examples[0][\"response\"]),\n",
        "        gr.Textbox(label=\"inquiry 2\", value=default_multi_shot_examples[1][\"inquiry\"]),\n",
        "        gr.Textbox(label=\"Response 2\", value=default_multi_shot_examples[1][\"response\"])\n",
        "\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Dataset\", lines=10, show_copy_button=True)\n",
        ")"
      ],
      "metadata": {
        "id": "SKEq-Ng9-acV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr_interface.launch(inbrowser=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "TTvyXBpP-f9C",
        "outputId": "13ce933d-cb04-4025-bed0-441c0fae97eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2df80bdb183246fd71.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2df80bdb183246fd71.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJbGZQ4xBTMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}